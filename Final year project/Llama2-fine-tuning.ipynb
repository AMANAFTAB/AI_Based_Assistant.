{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd679f1-9ec9-4379-a25d-2463c5d6936b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "58cd7a71-64b4-4cdc-ad4d-39a99361495d",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA GeForce RTX 4070 Laptop GPU with CUDA capability sm_89 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 4070 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /root/anaconda3/envs/tf did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=102, Highest Compute Capability: 8.9.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Required library version not found: libbitsandbytes_cuda102.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "\n",
      "================================================ERROR=====================================\n",
      "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
      "1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "2. CUDA driver not installed\n",
      "3. CUDA not installed\n",
      "4. You have multiple conflicting CUDA libraries\n",
      "5. Required library not pre-compiled for this bitsandbytes release!\n",
      "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
      "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
      "================================================================================\n",
      "\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n",
      "CUDA SETUP: Setup Failed!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullyShardedDataParallelPlugin, Accelerator\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfully_sharded_data_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullOptimStateDictConfig, FullStateDictConfig\n\u001b[1;32m      4\u001b[0m fsdp_plugin \u001b[38;5;241m=\u001b[39m FullyShardedDataParallelPlugin(\n\u001b[1;32m      5\u001b[0m     state_dict_config\u001b[38;5;241m=\u001b[39mFullStateDictConfig(offload_to_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rank0_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m     optim_state_dict_config\u001b[38;5;241m=\u001b[39mFullOptimStateDictConfig(offload_to_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rank0_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.21.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m skip_first_batches\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/accelerator.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SCALER_NAME,\n\u001b[1;32m     29\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     30\u001b[0m     get_pretty_name,\n\u001b[1;32m     31\u001b[0m     is_tpu_available,\n\u001b[1;32m     32\u001b[0m     is_xpu_available,\n\u001b[1;32m     33\u001b[0m     save,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_model\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxm\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/utils/__init__.py:131\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_deepspeed_available():\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m         DeepSpeedEngineWrapper,\n\u001b[1;32m    124\u001b[0m         DeepSpeedOptimizerWrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         HfDeepSpeedConfig,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    135\u001b[0m     _filter_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     prepare_tpu,\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/utils/bnb.py:42\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     find_tied_parameters,\n\u001b[1;32m     33\u001b[0m     get_balanced_memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     set_module_tensor_to_device,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m     47\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[1;32m     11\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/optim/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/cextension.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mgenerate_instructions()\n\u001b[1;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lib\u001b[38;5;241m.\u001b[39mget_context\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mc_void_p\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1de89fe-2171-4d61-b2ae-524e536a8a19",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "a289d9cc-b092-457b-aebb-3d6cca3acb28",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='family.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='family.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1510872b-13d2-420c-8dc1-97365d90a6c8",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5d203c5a-f8d5-4f6c-9971-7e83554c560d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "# def formatting_func(example):\n",
    "#     text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "#     return text\n",
    "  \n",
    "def formatting_func(example):\n",
    "    # Check if the 'note' key exists in the example object\n",
    "    if 'note' in example:\n",
    "        text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "    else:\n",
    "        # Handle the case where the 'note' key is not present\n",
    "        text = f\"### This example does not have a note.\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26d697c-afae-40d0-9a46-8e3cde4afbc2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "77dbe9fd-9451-4e74-a9da-e31baa901d0e",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=102, Highest Compute Capability: 8.9.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Required library version not found: libbitsandbytes_cuda102.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "\n",
      "================================================ERROR=====================================\n",
      "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
      "1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "2. CUDA driver not installed\n",
      "3. CUDA not installed\n",
      "4. You have multiple conflicting CUDA libraries\n",
      "5. Required library not pre-compiled for this bitsandbytes release!\n",
      "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
      "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
      "================================================================================\n",
      "\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n",
      "CUDA SETUP: Setup Failed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /root/anaconda3/envs/tf did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/modeling_utils.py:86\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_model, infer_auto_device_map, init_empty_weights\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     88\u001b[0m         check_tied_parameters_on_same_device,\n\u001b[1;32m     89\u001b[0m         find_tied_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m         set_module_tensor_to_device,\n\u001b[1;32m     95\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.21.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/accelerator.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SCALER_NAME,\n\u001b[1;32m     29\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     30\u001b[0m     get_pretty_name,\n\u001b[1;32m     31\u001b[0m     is_tpu_available,\n\u001b[1;32m     32\u001b[0m     is_xpu_available,\n\u001b[1;32m     33\u001b[0m     save,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/utils/__init__.py:131\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m         DeepSpeedEngineWrapper,\n\u001b[1;32m    124\u001b[0m         DeepSpeedOptimizerWrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         HfDeepSpeedConfig,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/accelerate/utils/bnb.py:42\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/optim/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/bitsandbytes/cextension.py:20\u001b[0m\n\u001b[1;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m base_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf_ZwxVlwqMfqsCMmqmalbKBzYUAjMzmMhoBc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    489\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 492\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:376\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 376\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:666\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    665\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:680\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:625\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1089\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0103c-bf4a-4106-80aa-8e57eb927657",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "82607078-38fe-4b86-a039-d53d00e80cf7",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    token = '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'\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aacd101-8b88-4dc9-b0e3-63b21cbc85bf",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "3935bfda-1f9d-4161-8a99-adc015d04b05",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772ce44-e81a-47f4-a5a1-d29739d5aa4e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "61d39175-c374-4355-9399-ec96fb90d430",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4f292c-3787-48a4-85b9-4b4f639a507e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5d268aae-2347-4917-b80a-a90402c70433",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "max_length = 512 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef66bd9a-6fe9-438f-82ad-793286a934e7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "fd0d526d-cdc4-4c35-932e-7e1890a39dbe",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491d56da354a49d08fc1e1f8a7ea2680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_and_tokenize_prompt2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenized_val_dataset \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt2)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/datasets/arrow_dataset.py:3517\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3515\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3517\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mgenerate_and_tokenize_prompt2\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_and_tokenize_prompt2\u001b[39m(prompt):\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(\n\u001b[1;32m      5\u001b[0m         formatting_func(prompt),\n\u001b[1;32m      6\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m      8\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b2f98a-9ec5-44c5-97f6-eedb4ff660de",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "0fb19e2d-a659-436d-9fd2-24fc2a2eecfd",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 910, 1342, 947, 451, 505, 263, 4443, 29889, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e458310-78e8-41da-826a-0ab7acfb5509",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "4e63580f-422b-4866-b7a8-45a812469d63",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68867410-ee45-43e1-9d2a-28baf1825243",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "a7d2c75b-2d6a-40bf-811a-36c43c2c890f",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\" Given the following biometric data, score the users' health, from 0-100.\n",
    "\n",
    "### Biometric Data:\n",
    "Temperature=98.2,\n",
    "Sex=F,\n",
    "Age=29,\n",
    "Height=69 inches,\n",
    "Weight=160 lbs,\n",
    "V02_Max=55,\n",
    "HRV=55\n",
    "\n",
    "### Health Score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca12ae07-063c-4d80-bb3a-1fd5ab5cf80b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "14fec2b9-6dca-4960-8280-9491a5ead755",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"Who is emily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f7de3b9-d2aa-466f-ad0b-a9a84896d052",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "3f7a03b9-d875-4e7c-af92-021e5e0d0066",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is emily. ### This example have a note. ### This example ### ### This example ### This example This ### This example ### This example ### This example ### This example example example example a example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example This example This example This example This example This example This example This example This example This example example example This example This example This example This example This example This example This example This example This example example This example a example example a example example example example example example example a example example example a example example example a example example This example a example example example example a example example example example example example example a example example example example example example example example This example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c411fb5f-3108-4ed1-b3f2-e0579391cba6",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "34e08236-5931-4965-886e-d08effd8fc8d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ff972b0-8771-4b20-9093-bd0368af5ec1",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "4446fc91-e7a1-44e4-8da9-8da28621b299",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57712f81-4aca-477a-a662-580fdb35a04f",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "c2fdce7b-699f-48c0-a04c-dda685144044",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c0e74ea-beb2-40b2-bc99-ad91a1079c25",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5a7d43dc-5362-4932-ba06-a9453eaa9518",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 81108992 || all params: 3581521920 || trainable%: 2.264651559077991\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60b0adef-ec1f-4a6e-bd8a-93925c108f81",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "6a4762c7-567a-47de-b5bc-b39931129641",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=4096, out_features=32000, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585f46a4-0a8c-4355-8bb1-ce62590ce7a5",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "8701086f-2a5a-4685-a4bb-fa94cea234d8",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89e3d153-1c0d-4e8e-8573-7235582f30f8",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "7156c53f-c476-429a-a2df-a28a1ed65f3b",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 08:37, Epoch 37/38]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.006279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>15.817639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.159014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=6.983985188802083, metrics={'train_runtime': 521.4956, 'train_samples_per_second': 0.575, 'train_steps_per_second': 0.288, 'total_flos': 3179934646272000.0, 'train_loss': 6.983985188802083, 'epoch': 37.5})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"patient-finetune\"\n",
    "base_model_name = \"llama2-7b-chat-hf\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=150,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        # report_to=,           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e08d98-cf23-4f8f-ab44-4e6ef3a1b5ce",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "bb07ca42-d3ed-4f74-a8a8-ed9330cc7e78",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f46f48051845b8bd4963cfaa11cd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Llama 2 7B, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    # device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f5cf44d-0e0b-4f3a-8faf-26f1862992b2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5d1a9b10-9fc0-475f-b790-71aee6fea533",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"llama2-7b-chat-hf-patient-finetune/checkpoint-150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acad885-5454-455b-8bb6-8ea5d904403c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "7bb96a5d-dadd-4a3a-b010-bf4af2fc3258",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Who is emily\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# eval_prompt = \"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(eval_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m ft_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" Who is emily\"\n",
    "# eval_prompt = \"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0342142d-1de7-4dad-b1a9-56c0df846f8f",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "81271adb-4d69-41ae-aeff-f12dd6bed7b1",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/generation/utils.py:2678\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2677\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2678\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" Who is emily\"\n",
    "# eval_prompt = \"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066e4ec-70c5-44b6-ad95-e54f4f26cbf1",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "662dfba1-9ac5-4811-bb05-48c452888a0c",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 22:36:24.383169: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-01 22:36:24.619901: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-01 22:36:24.619948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-01 22:36:24.656756: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-01 22:36:24.733941: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 22:36:25.548491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
