{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96ee093-bd58-426d-9e25-bf0ad7b13f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 22:46:11.861031: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-05 22:46:11.941482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 22:46:12.806782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/anaconda3/envs/final_year/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader,ServiceContext,VectorStoreIndex\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import warnings\n",
    "\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b718163-57b5-4f73-8f1b-c3a6951f8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rag_Llama:\n",
    "    def __init__(self,\n",
    "                context_window=4096,\n",
    "                max_new_tokens=256,\n",
    "                generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "                system_prompt=\"\"\"\"\"\",\n",
    "                tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                device_map=\"cuda:0\",\n",
    "                model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}):\n",
    "        \n",
    "        self.context_window= context_window\n",
    "        self.max_new_tokens= max_new_tokens\n",
    "        self.generate_kwargs= generate_kwargs\n",
    "        self.system_prompt=system_prompt\n",
    "        # query_wrapper_prompt=query_wrapper_prompt,\n",
    "        self.tokenizer_name= tokenizer_name\n",
    "        self.model_name= model_name\n",
    "        self.device_map= device_map\n",
    "        # uncomment this if using CUDA to reduce memory usage\n",
    "        self.model_kwargs= model_kwargs\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        self.llm = HuggingFaceLLM(\n",
    "                context_window= self.context_window,\n",
    "                max_new_tokens= self.max_new_tokens,\n",
    "                generate_kwargs= self.generate_kwargs,\n",
    "                system_prompt= self.system_prompt,\n",
    "                # query_wrapper_prompt=query_wrapper_prompt,\n",
    "                tokenizer_name= self.tokenizer_name,\n",
    "                model_name= self.model_name,\n",
    "                device_map= self.device_map,\n",
    "                # uncomment this if using CUDA to reduce memory usage\n",
    "                model_kwargs= self.model_kwargs,\n",
    "                # llm_int8_enable_fp32_cpu_offload=True\n",
    "            )\n",
    "\n",
    "    def load_data(self, data_path = \"./data\"):\n",
    "        # try:\n",
    "        self.documents=SimpleDirectoryReader(\"./data\").load_data()\n",
    "        if self.documents:\n",
    "            print(\"Documents Loaded\")\n",
    "        else:\n",
    "            print(\"No Documents found, please check the path or the document format \\n The document format must be in pdf\")\n",
    "        # except:\n",
    "        #     print(\"Error in loading document, Simple Directory Error\")\n",
    "\n",
    "    \n",
    "    def call(self, query, embedding_model = \"sentence-transformers/all-mpnet-base-v2\", data_path = \"./data\", first = True):\n",
    "            if first:\n",
    "                self.load_model()\n",
    "                self.load_data(data_path)\n",
    "                self.embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name= embedding_model))\n",
    "\n",
    "                self.service_context=ServiceContext.from_defaults(\n",
    "                    chunk_size=1024,\n",
    "                    llm=self.llm,\n",
    "                    embed_model=self.embed_model\n",
    "                )\n",
    "\n",
    "                self.system_prompt =\"\"\"\n",
    "                You are a human being that is trying to converse with an \n",
    "                Alzheimer's patient. \n",
    "                Use the memories in the data and respond naturally.\n",
    "                \"\"\"\n",
    "                \n",
    "                self.index=VectorStoreIndex.from_documents(self.documents, service_context = self.service_context)\n",
    "                self.query_engine=self.index.as_query_engine()\n",
    "                \n",
    "                \n",
    "                \n",
    "                return self.query_engine\n",
    "                \n",
    "    \n",
    "            # else:\n",
    "            #     self.load_data(data_path)\n",
    "            #     self.index=VectorStoreIndex.from_documents(self.documents, service_context = self.service_context)\n",
    "            #     self.query_engine=self.index.as_query_engine()\n",
    "                \n",
    "            #     self.response=self.query_engine.query(query)\n",
    "                \n",
    "            #     return self.response\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb30eb2-5b09-4c0b-ae01-5d454e892d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = Rag_Llama(context_window=4096,\n",
    "#                 max_new_tokens=256,\n",
    "#                 generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "#                 system_prompt=\"\"\"\"\"\",\n",
    "#                 tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                 model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                 device_map=\"cuda:0\",\n",
    "#                 model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True})\n",
    "# query_engine = obj.call(\"How did the camping trip go?\", embedding_model = \"sentence-transformers/all-mpnet-base-v2\", data_path = \"./data\", first = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1061feb-17e7-470d-af97-953a38f84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, query_engine):\n",
    "        response=query_engine.query(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb9b26f-9ad2-4f12-a1a7-fd8e1970be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response=get_response(\"How is the trip going?\", query_engine)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48562c4b-1783-4024-a3b6-5d30ff545a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain_Llama:\n",
    "\n",
    "    def __init__(self,\n",
    "                model = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "        self.model = model # meta-llama/Llama-2-7b-hf\n",
    "\n",
    "    \n",
    "    def load_plain_model_pipeline(self, device_map=\"cuda:0\"):\n",
    "        self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model, device_map = device_map, load_in_8bit=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model, use_auth_token=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "        self.llama_pipeline = pipeline(\n",
    "            \"text-generation\",  # LLM task\n",
    "            model=self.model_8bit,\n",
    "            tokenizer = self.tokenizer,\n",
    "            torch_dtype=torch.uint8,\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "    \n",
    "    def get_plain_llama_response(self,prompt: str) -> None:\n",
    "        \"\"\"\n",
    "        Generate a response from the Llama model.\n",
    "    \n",
    "        Parameters:\n",
    "            prompt (str): The user's input/question for the model.\n",
    "    \n",
    "        Returns:\n",
    "            None: Prints the model's response.\n",
    "        \"\"\"\n",
    "        self.sequences = self.llama_pipeline(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            max_length=256,\n",
    "        )\n",
    "    \n",
    "        return self.sequences[0]\n",
    "\n",
    "    def call_plain_model(self, prompt = \"\"):\n",
    "        self.load_plain_model_pipeline()\n",
    "        \n",
    "        self.prompt = prompt\n",
    "        self.plain_response = self.get_plain_llama_response(self.prompt)\n",
    "\n",
    "        # self.model_8bit.cpu()\n",
    "        # del self.model_8bit, checkpoint\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        return self.plain_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01ca704-f1b6-4534-8e21-fea7006b7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj2 = Plain_Llama(model = \"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# response = obj2.call_plain_model(prompt = \"How did the camping trip go?\")\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ec507-a07e-478d-86ee-8897761bc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents=SimpleDirectoryReader(\"./data\").load_data()\n",
    "# documents\n",
    "\n",
    "# system_prompt=\"\"\"\n",
    "# You are a Q&A assistant. Your goal is to answer questions as\n",
    "# accurately as possible based on the instructions and context provided.\n",
    "# \"\"\"\n",
    "# ## Default format supportable by LLama2\n",
    "# # query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "# # llm = HuggingFaceLLM(\n",
    "# #     context_window=4096,\n",
    "# #     max_new_tokens=256,\n",
    "# #     generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "# #     system_prompt=system_prompt,\n",
    "# #     # query_wrapper_prompt=query_wrapper_prompt,\n",
    "# #     tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "# #     model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "# #     device_map=\"cuda:0\",\n",
    "# #     # uncomment this if using CUDA to reduce memory usage\n",
    "# #     model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True},\n",
    "# #     # llm_int8_enable_fp32_cpu_offload=True\n",
    "# # )\n",
    "\n",
    "# embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "\n",
    "# service_context=ServiceContext.from_defaults(\n",
    "#     chunk_size=1024,\n",
    "#     llm=llm,\n",
    "#     embed_model=embed_model\n",
    "# )\n",
    "\n",
    "# index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "# query_engine=index.as_query_engine()\n",
    "\n",
    "# response=query_engine.query(\"what happened during the camping trip?\")\n",
    "\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
